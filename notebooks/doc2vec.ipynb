{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec, JUMAN++, KNPで文書類似度算出\n",
    "\n",
    "## 参考：\n",
    "\n",
    "- [DeepAge Doc2Vecの仕組みとgensimを使った文書類似度算出チュートリアル](https://deepage.net/machine_learning/2017/01/08/doc2vec.html#最も似ている記事を取得する)\n",
    "- [黒橋・河原研究室　自然言語処理のためのリソース](http://nlp.ist.i.kyoto-u.ac.jp/index.php?NLP%E3%83%AA%E3%82%BD%E3%83%BC%E3%82%B9)\n",
    "- [hassaku's blog 学習済みword2vecモデルを調べてみた](http://blog.hassaku-labs.com/post/pretrained-word2vec/)\n",
    "- [白ヤギコーポレーション word2vecの学習済み日本語モデルを公開します](http://aial.shiroyagi.co.jp/2017/02/japanese-word2vec-model-builder/)\n",
    "\n",
    "## インストール\n",
    "\n",
    "gensim (doc2vec), JUMAN, JUMAN++, KNPを組み込んだDockerを起動\n",
    "\n",
    "    docker-compose up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コーパス\n",
    "\n",
    "livedoorのニュースコーパスをダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-06-05 16:19:49--  http://www.rondhuit.com/download/.tar.gz\n",
      "Resolving www.rondhuit.com (www.rondhuit.com)... 59.106.19.174\n",
      "Connecting to www.rondhuit.com (www.rondhuit.com)|59.106.19.174|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://www.rondhuit.com/download/.tar.gz [following]\n",
      "--2017-06-05 16:19:49--  https://www.rondhuit.com/download/.tar.gz\n",
      "Connecting to www.rondhuit.com (www.rondhuit.com)|59.106.19.174|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2017-06-05 16:19:51 ERROR 404: Not Found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!LDCC=ldcc-20140209 \\\n",
    " wget http://www.rondhuit.com/download/$LDCC.tar.gz \\\n",
    " && tar xvfz $LDCC.tar.gz \\\n",
    " && rm $LDCC.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 助走\n",
    "\n",
    "必要ライブラリをimportし、関数をいくつか定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from os import listdir, path\n",
    "from pyknp import Jumanpp\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "記事ファイルをダウンロードしたディレクトリから取得する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corpus_files():\n",
    "    dirs = [path.join('text', x)\n",
    "            for x in listdir('text') if not x.endswith('.txt')]\n",
    "    docs = [path.join(x, y)\n",
    "            for x in dirs for y in listdir(x) if not x.startswith('LICENSE')]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "記事コンテンツをパスから取得する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_document(path):\n",
    "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JUMAN++を使って記事を単語リストに変換する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_into_words(text):\n",
    "    result = Jumanpp().analysis(text)\n",
    "    return [mrph.midasi for mrph in result.mrph_list()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 形態素解析\n",
    "\n",
    "記事コンテンツを単語に分割して、Doc2Vecの入力に使うLabeledSentenceに変換する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc_to_sentence(doc, name):\n",
    "    words = split_into_words(doc)\n",
    "    return LabeledSentence(words=words, tags=[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "記事のパスリストから、記事コンテンツに変換し、単語分割して、センテンスのジェネレーターを返す関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corpus_to_sentences(corpus):\n",
    "    docs   = [read_document(x) for x in corpus]\n",
    "    for idx, (doc, name) in enumerate(zip(docs, corpus)):\n",
    "        sys.stdout.write('\\r前処理中 {}/{}'.format(idx, len(corpus)))\n",
    "        yield doc_to_sentence(doc, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vecパラメータを渡して、学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = corpus_files()\n",
    "sentences = corpus_to_sentences(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "\n",
    "dmに1を設定するとdmpvで学習されることになる。1以外であれば、DBoWで学習される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前処理中 7375/7376"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(sentences, dm=0, size=300, window=15, alpha=.025,\n",
    "        min_alpha=.025, min_count=1, sample=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "訓練開始\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n"
     ]
    }
   ],
   "source": [
    "print('\\n訓練開始')\n",
    "for epoch in range(20):\n",
    "    print('Epoch: {}'.format(epoch + 1))\n",
    "    model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    model.alpha -= (0.025 - 0.0001) / 19\n",
    "    model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの保存と読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p models\n",
    "model.save('models/doc2vec.model')\n",
    "model = Doc2Vec.load('models/doc2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 類似記事の取得\n",
    "\n",
    "\n",
    "最も似ている記事を取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/it-life-hack/it-life-hack-6802274.txt', 0.20245397090911865)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar('text/livedoor-homme/livedoor-homme-5625149.txt', topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 類似度の算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.057172305823492496"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.similarity('text/livedoor-homme/livedoor-homme-4700669.txt', 'text/movie-enter/movie-enter-5947726.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
